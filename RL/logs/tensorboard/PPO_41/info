Algorithme PPO:
- training steps -> 1_000_000
- episode max_steps -> 800
- time.sleep(0.1)

Observation -> Convergence vers un maximum (Le meme que pour le SAC) mod√®le qui ne fonctionne pas en test
import time
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from control import BlueROVEnv  # Ton environnement Gym ROS2
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.monitor import Monitor

# Wrapper pour lâ€™environnement (nÃ©cessaire pour stable-baselines3)
def make_env():
    return BlueROVEnv()

def train_model():
    print("ðŸš€ EntraÃ®nement PPO du BlueROV en cours...")

    # Environnement d'entraÃ®nement monitorÃ©
    train_env = DummyVecEnv([lambda: Monitor(BlueROVEnv())])

    model = PPO(
        policy="MlpPolicy",
        env=train_env,
        verbose=1,
        tensorboard_log="./logs/tensorboard/",
        device="cpu",  # Pour Ã©viter l'avertissement GPU inutile
        ent_coef=0.001
    )

    # DÃ©marrage de l'apprentissage
    model.learn(total_timesteps=200_000)  
    # Sauvegarde du modÃ¨le
    model.save("./logs/final_model_bluerov_ppo")
    print("âœ… ModÃ¨le entraÃ®nÃ© et sauvegardÃ©.")

def test_model():
    print("ðŸ§ª Test du modÃ¨le PPO sur BlueROV...")

    env = DummyVecEnv([make_env])
    model = PPO.load("ppo_bluerov")

    num_episodes = 10

    for episode in range(num_episodes):
        print(f"\nðŸŽ¯ Ã‰pisode {episode + 1} / {num_episodes}")
        obs = env.reset()
        done = False
        truncated = False
        total_reward = 0
        step = 0

        while not (done or truncated):
            action, _ = model.predict(obs, deterministic=True)

            obs, reward, done, _ = env.step(action)

            total_reward += reward[0]
            step += 1
            print(f"ðŸ”¹ Step {step}: Reward={reward[0]:.2f}, Done={done}, Truncated={truncated}")

            time.sleep(0.1)

        print(f"âœ… Fin de lâ€™Ã©pisode {episode + 1} - Total Reward: {total_reward:.2f}, Steps: {step}")
        time.sleep(1)

    env.close()

if __name__ == "__main__":
    train_model()
    #test_model()
